```markdown
# ğŸš— Used Car Price Prediction (Pakistan Market)

This project predicts **used car prices** based on scraped data from [Pakwheels](https://www.pakwheels.com/used-cars/).  
It demonstrates the **Data Science workflow**:  
data collection â†’ cleaning â†’ EDA â†’ feature engineering â†’ model building â†’ evaluation.  

---

## ğŸ“‚ Project Workflow  

### 1. Web Scraping  
- Scraped car listings (title, price, year, mileage, fuel, transmission, engine, etc.)  
- Implemented pagination to scrape multiple pages.  
- Handled failed requests and optimized scraping for Colab environment.  
- Saved raw dataset as CSV for reproducibility.  

### 2. Data Cleaning  
- Removed duplicates & irrelevant columns (e.g., title).  
- Fixed column types (numeric conversion for price, mileage, engine size).  
- Handled missing values:
  - `engine_cc`, `battery_kwh` â†’ imputed with `0` + missingness flag.  
  - No nulls in categorical columns.  
- Handled unrealistic values:
  - Mileage > 1,000,000 km â†’ capped/dropped.  
  - Price spikes for old models (e.g., vintage cars) â†’ handled as outliers.  

### 3. Exploratory Data Analysis (EDA)  
- Distribution plots of price, mileage, year.  
- Price vs Mileage scatterplots â†’ identified depreciation trends.  
- Heatmap of numeric correlations.  
- Grouped analysis:
  - Price variation across **brands** (company).  
  - Price variation across **models**.  
- Detected multicollinearity with correlation & VIF.  

### 4. Feature Engineering  
- **Derived features:**
  - `car_age = CurrentYear - Year`  
  - `price_per_km = Price / Mileage`  
- **Transformations for skewed data:**  
  - Log / Box-Cox for mileage and price.  
- **Encoding:**  
  - One-Hot â†’ fuel, transmission, engine_type, company  
  - Frequency Encoding â†’ model  
- **Scaling:**  
  - RobustScaler for numeric features  

### 5. Modeling  
- Baseline models: Linear, Ridge, Lasso, ElasticNet  
- Tree-based models: Decision Tree, Random Forest  
- Boosting: AdaBoost, XGBoost, LightGBM  
- **Best model:**  
  - **LightGBM (tuned)**  
  - Cross-validated RÂ²: ~0.90  
  - MAE: ~5.1 lacs   

---

## ğŸ“Š Results  

| Model              | MAE (â†“) | RÂ² (â†‘) |
|--------------------|---------|--------|
| Linear Regression  | 16.3    | 0.61   |
| Decision Tree      | 7.0     | 0.85   |
| Random Forest      | 7.3     | 0.86   |
| XGBoost            | 5.2     | 0.89   |
| **LightGBM (tuned)** | **5.1** | **0.90** |

---

## âš™ï¸ Tech Stack  

- **Data Collection:** BeautifulSoup, Requests  
- **Data Processing:** pandas, numpy  
- **EDA & Visualization:** matplotlib, seaborn  
- **Modeling:** scikit-learn, XGBoost, LightGBM    

---

## ğŸ“‚ Repository Structure  

```

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.csv
â”‚   â””â”€â”€ cleaned_data.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_webscraping.ipynb
â”‚   â”œâ”€â”€ 02_cleaning_eda.ipynb
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 04_modeling.ipynb
â”‚   â””â”€â”€ 05_deployment.ipynb
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ final_lgbm_pipeline.pkl
â””â”€â”€ README.md

```
---

## ğŸ”‘ Key Learnings  

- Designing end-to-end ML pipelines for real-world data.  
- Handling imbalanced categories (rare companies/models).  
- Encoding strategies for high-cardinality features.  
- Comparing classical regression vs tree-based models.  
- Hyperparameter tuning for boosting algorithms.  

---

## ğŸ‘¨â€ğŸ’» Author  

**Basharat Asghar**  
- Data Scientist | ML Enthusiast  
- [LinkedIn](https://www.linkedin.com/in/basharat-asghar) | [GitHub]([https://github.com](https://github.com/Basharat-Asghar))  

---
```
